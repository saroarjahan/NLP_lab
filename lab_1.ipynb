{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a86d5e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________Using TFIDF_____________\n",
      "City of Oulu\n",
      "___________________________\n",
      "______________Using BOW______________\n",
      "City of Helsinki\n",
      "___________________________\n"
     ]
    }
   ],
   "source": [
    "# Lab 1: A â€“ Information Retrieval\n",
    "\n",
    "#  Sample by Y. Bounab'\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Resources for scrabing data -- Two methods are highlighted here beatiful soup or using Wikipedia API\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "\n",
    "'https://levelup.gitconnected.com/two-simple-ways-to-scrape-text-from-wikipedia-in-python-9ce07426579b'\n",
    "'https://www.datacamp.com/community/tutorials/stemming-lemmatization-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=255798340456&utm_targetid=aud-299261629574:dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1005766&gclid=CjwKCAjwwab7BRBAEiwAapqpTN9AkZuHgQZK__Jj6Wrkh_2xHt3DUGLkdURgxSetxlRFJ9xY3XVwkRoCD-0QAvD_BwE'\n",
    "\n",
    "BaseLink = 'https://en.wikipedia.org'\n",
    "\n",
    "Q = 'I wanted to visit Oulu this summer if possible, otherwise I will go to Helsinki'\n",
    "\n",
    "Finnish_Cities = {}\n",
    "\n",
    "def WIKI(title):\n",
    "    wiki = wikipedia.page(title)\n",
    "    text = wiki.content\n",
    "    text = text.replace('==', '')\n",
    "\n",
    "    text = text.replace('\\n', '')[:-12]\n",
    "    return text\n",
    "\n",
    "def get_Text(link): # This fucntion will get all\n",
    "    soup = BeautifulSoup(urlopen(link),'lxml')\n",
    "    soup = soup.find('div', id='mw-content-text')#.find('div',)\n",
    "    Text = ''\n",
    "    for item in soup.find_all('p'):#, recursive=False):\n",
    "        Text += item.text.strip()\n",
    "    return Text\n",
    "\n",
    "def get_Finnish_Cities(): # This fucntion will get all finnish city name\n",
    "    soup = BeautifulSoup(urlopen('https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Finland'),'lxml')\n",
    "    soup = soup.find('div',class_='mw-parser-output')\n",
    "    Table = soup.find('table', style=\"text-align:right;\").find('tbody')\n",
    "    \n",
    "    for item in Table.find_all('tr'):\n",
    "        if item.find('td'):\n",
    "           name = item.find('td').find('a')\n",
    "           if name:\n",
    "              Finnish_Cities[name['title']] = get_Text(BaseLink+name['href'])\n",
    "           else:\n",
    "               name = item.find_all('td')[1].find('a')\n",
    "               Finnish_Cities[name['title']] = get_Text(BaseLink+name['href'])\n",
    "               \n",
    "        \n",
    "def preProcess(doc): # This fucntion remove stop words and perform stemming and lamentization\n",
    "    Stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    WN_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    sentences = sent_tokenize(doc)\n",
    "    Tokens = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        words = [WN_lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "        \n",
    "        words = [word for word in words if word.isalpha() and word not in Stopwords] #get rid of numbers and Stopwords\n",
    "        #words= [word for word in words if word.isalnum() and word not in Stopwords]\n",
    "        Tokens.extend(words)\n",
    "        \n",
    "    return ' '.join(Tokens)\n",
    "    \n",
    "def BOW_model(corpus): # This fucntion will transform document to countVector matrix\n",
    "    BOW = CountVectorizer(preprocessor=preProcess)    #max_df=0.8, min_df=0.2, )  in case you want to reduce the size of the dictionary, you can change the default values of max_de, min_def and other parameters\n",
    "    BOW.fit(corpus)\n",
    "    #X = BOW.transform(corpus)\n",
    "    #X = BOW.fit_transform(corpus)\n",
    "    return BOW\n",
    "\n",
    "def TFIDF(corpus): # This fucntion will transform document to tf-idf matrix\n",
    "    Tfidf = TfidfVectorizer(preprocessor=preProcess)\n",
    "    Tfidf.fit(corpus)\n",
    "#    print(Tfidf.get_feature_names())\n",
    "#    print(Tfidf.vocabulary_)\n",
    "\n",
    "    #X = Tfidf.transform(corpus)\n",
    "    #X = Tfidf.fit_transform(corpus)\n",
    "    return Tfidf\n",
    "\n",
    "def Answering_Q(Q, Vectorizer): # This fucntion is for refering the city name that will match with query. You can also print documnet\n",
    "    Vectors = Vectorizer.transform(list(Finnish_Cities.values())).toarray()\n",
    "    Vq = Vectorizer.transform([Q]).toarray()[0]\n",
    "    \n",
    "    Scores = []\n",
    "    for V in Vectors:\n",
    "        Scores.append(np.inner(Vq, V))\n",
    "        \n",
    "    Max_index = Scores.index(max(Scores))\n",
    "    print('City of',list(Finnish_Cities)[Max_index])\n",
    "    print('___________________________')\n",
    "    #print(Finnish_Cities[list(Finnish_Cities)[Max_index]])\n",
    "    \n",
    "    \n",
    "get_Finnish_Cities()\n",
    "\n",
    "Tfidf = TFIDF(list(Finnish_Cities.values()))\n",
    "BOW = BOW_model(list(Finnish_Cities.values()))\n",
    "\n",
    "print('_____________Using TFIDF_____________')\n",
    "Answering_Q(Q, Tfidf)\n",
    "\n",
    "print('______________Using BOW______________')\n",
    "Answering_Q(Q, BOW)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd780b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
